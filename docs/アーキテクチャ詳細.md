# ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°

## ğŸ“‹ **ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“æ¦‚è¦**

ãƒ‹ãƒ¥ãƒ¼ã‚¹é…ä¿¡ã‚·ã‚¹ãƒ†ãƒ ã¯ã€**ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹å‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**ã¨**ã‚¤ãƒ™ãƒ³ãƒˆé§†å‹•å‹è¨­è¨ˆ**ã‚’æ¡ç”¨ã—ãŸé«˜åº¦ãªè‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚

### **è¨­è¨ˆæ€æƒ³**
- **ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒªãƒ†ã‚£**: å„æ©Ÿèƒ½ã‚’ç‹¬ç«‹ã—ãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦è¨­è¨ˆ
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: å®¹æ˜“ãªæ©Ÿèƒ½è¿½åŠ ãƒ»æ‹¡å¼µãŒå¯èƒ½
- **ä¿¡é ¼æ€§**: å†—é•·åŒ–ã¨ã‚¨ãƒ©ãƒ¼è€æ€§ã‚’é‡è¦–
- **è‡ªå‹•åŒ–**: GitHub Actionsã«ã‚ˆã‚‹å®Œå…¨CI/CDçµ±åˆ

---

## ğŸ”„ **ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³**

### **ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**

```mermaid
graph TB
    subgraph "Presentation Layer"
        A[Cron Scheduler] --> B[Main Controller]
        B --> C[Error Handler]
    end
    
    subgraph "Business Logic Layer"
        D[News Collectors] --> E[Data Processors]
        E --> F[Report Generators]
        F --> G[Notification System]
    end
    
    subgraph "Data Access Layer"
        H[SQLite Database]
        I[Cache Manager]
        J[File Storage]
    end
    
    subgraph "External APIs"
        K[NewsAPI]
        L[DeepL API]
        M[Claude API]
        N[Gmail API]
        O[NVD API]
    end
    
    subgraph "Infrastructure Layer"
        P[GitHub Actions]
        Q[Security Scanners]
        R[Monitoring]
    end
    
    B --> D
    D --> K
    D --> O
    E --> L
    E --> M
    G --> N
    D -.-> H
    E -.-> I
    F -.-> J
```

### **ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼è©³ç´°**

```mermaid
sequenceDiagram
    participant Cron
    participant Main
    participant Collectors
    participant Processors
    participant Generators
    participant Notifiers
    participant DB
    
    Cron->>Main: Trigger (7:00/12:00/18:00)
    Main->>Collectors: Start Collection
    
    par Parallel Collection
        Collectors->>NewsAPI: Fetch Articles
        Collectors->>NVD: Fetch Vulnerabilities
        Collectors->>GNews: Fetch Tech News
    end
    
    Collectors->>DB: Store Raw Articles
    Collectors->>Main: Return Articles
    
    Main->>Processors: Process Articles
    
    par Parallel Processing
        Processors->>DeepL: Translate Content
        Processors->>Claude: Analyze & Summarize
        Processors->>Processors: Deduplicate
    end
    
    Processors->>DB: Store Processed Data
    Processors->>Main: Return Processed Articles
    
    Main->>Generators: Generate Reports
    Generators->>Generators: Create HTML
    Generators->>Generators: Create PDF
    Generators->>Main: Return Report Paths
    
    Main->>Notifiers: Send Notifications
    Notifiers->>Gmail: Send Email
    Notifiers->>Main: Confirm Delivery
    
    Main->>DB: Log Results
```

---

## ğŸ§© **ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è©³ç´°è¨­è¨ˆ**

### **1. ğŸ—‚ï¸ Collectors (ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å±¤)**

#### **BaseCollector (æŠ½è±¡åŸºåº•ã‚¯ãƒ©ã‚¹)**
```python
class BaseCollector(ABC):
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å™¨ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, api_key: str, cache_manager: CacheManager):
        self.api_key = api_key
        self.cache = cache_manager
        self.session = None
        
    @abstractmethod
    async def collect(self, category: str, count: int) -> List[Article]:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã®å®Ÿè£…"""
        pass
        
    async def fetch_with_cache(self, url: str, params: dict) -> dict:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
        cache_key = self._generate_cache_key(url, params)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
        cached_data = await self.cache.get(cache_key)
        if cached_data:
            return cached_data
            
        # APIå‘¼ã³å‡ºã—
        async with self.session.get(url, params=params) as response:
            data = await response.json()
            
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        await self.cache.set(cache_key, data, ttl=3600)
        return data
```

#### **NewsAPICollector (NewsAPIé€£æº)**
```python
class NewsAPICollector(BaseCollector):
    """NewsAPIå°‚ç”¨åé›†å™¨"""
    
    BASE_URL = "https://newsapi.org/v2"
    RATE_LIMIT = 1000  # requests/day
    
    async def collect(self, category: str, count: int) -> List[Article]:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†"""
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
        if await self._check_rate_limit():
            raise RateLimitError("NewsAPI rate limit exceeded")
            
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ§‹ç¯‰
        params = self._build_params(category, count)
        
        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        data = await self.fetch_with_cache(
            f"{self.BASE_URL}/everything", 
            params
        )
        
        # Article ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå¤‰æ›
        articles = []
        for article_data in data.get('articles', []):
            article = Article(
                title=article_data.get('title'),
                description=article_data.get('description'),
                url=article_data.get('url'),
                source=article_data.get('source', {}).get('name'),
                published_at=article_data.get('publishedAt'),
                category=category,
                raw_data=article_data
            )
            articles.append(article)
            
        return articles
        
    def _build_params(self, category: str, count: int) -> dict:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç”Ÿæˆ"""
        base_params = {
            'apiKey': self.api_key,
            'pageSize': count,
            'sortBy': 'relevancy',
            'language': 'ja' if 'domestic' in category else 'en'
        }
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥è¨­å®š
        category_config = {
            'domestic_social': {
                'country': 'jp',
                'category': 'general'
            },
            'international_social': {
                'q': 'human rights OR social justice OR migration',
                'domains': 'bbc.co.uk,reuters.com,ap.org'
            },
            'tech': {
                'q': 'artificial intelligence OR machine learning OR technology',
                'domains': 'techcrunch.com,arstechnica.com,wired.com'
            },
            'security': {
                'q': 'cybersecurity OR vulnerability OR data breach',
                'domains': 'krebsonsecurity.com,threatpost.com'
            }
        }
        
        base_params.update(category_config.get(category, {}))
        return base_params
```

### **2. âš™ï¸ Processors (ãƒ‡ãƒ¼ã‚¿å‡¦ç†å±¤)**

#### **TranslationProcessor (ç¿»è¨³å‡¦ç†)**
```python
class DeepLTranslator:
    """DeepL APIç¿»è¨³å‡¦ç†"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api-free.deepl.com/v2/translate"
        self.batch_size = 50
        self.cache = CacheManager()
        
    async def translate_batch(self, articles: List[Article]) -> List[Article]:
        """ãƒãƒƒãƒç¿»è¨³å‡¦ç†"""
        
        # ç¿»è¨³å¯¾è±¡è¨˜äº‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        translation_targets = [
            article for article in articles 
            if self._needs_translation(article)
        ]
        
        # ãƒãƒƒãƒå‡¦ç†
        batches = self._create_batches(translation_targets, self.batch_size)
        
        tasks = []
        for batch in batches:
            task = asyncio.create_task(self._translate_batch(batch))
            tasks.append(task)
            
        # ä¸¦åˆ—å®Ÿè¡Œ
        translated_batches = await asyncio.gather(*tasks)
        
        # çµæœãƒãƒ¼ã‚¸
        translated_articles = []
        for batch in translated_batches:
            translated_articles.extend(batch)
            
        return translated_articles
        
    async def _translate_batch(self, articles: List[Article]) -> List[Article]:
        """å˜ä¸€ãƒãƒƒãƒã®ç¿»è¨³"""
        
        # ç¿»è¨³ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        texts = []
        for article in articles:
            texts.extend([article.title, article.description or ""])
            
        # DeepL APIå‘¼ã³å‡ºã—
        async with aiohttp.ClientSession() as session:
            async with session.post(
                self.base_url,
                headers={'Authorization': f'DeepL-Auth-Key {self.api_key}'},
                data={
                    'text': texts,
                    'source_lang': 'EN',
                    'target_lang': 'JA',
                    'formality': 'default'
                }
            ) as response:
                result = await response.json()
                
        # ç¿»è¨³çµæœé©ç”¨
        translations = result['translations']
        for i, article in enumerate(articles):
            article.translated_title = translations[i*2]['text']
            article.translated_description = translations[i*2+1]['text']
            
        return articles
```

#### **AIAnalyzer (AIåˆ†æå‡¦ç†)**
```python
class ClaudeAnalyzer:
    """Claude APIåˆ†æå‡¦ç†"""
    
    def __init__(self, api_key: str):
        self.client = anthropic.Client(api_key=api_key)
        self.cache = CacheManager()
        self.semaphore = asyncio.Semaphore(5)  # ä¸¦è¡Œæ•°åˆ¶é™
        
    async def analyze_batch(self, articles: List[Article]) -> List[Article]:
        """ãƒãƒƒãƒAIåˆ†æ"""
        
        # é‡è¦åº¦ã§å„ªå…ˆé †ä½ä»˜ã‘
        prioritized = self._prioritize_articles(articles)
        
        # é«˜å„ªå…ˆåº¦è¨˜äº‹ã¯Claudeåˆ†æ
        high_priority = prioritized[:20]
        low_priority = prioritized[20:]
        
        # ä¸¦åˆ—åˆ†æå®Ÿè¡Œ
        tasks = []
        for article in high_priority:
            task = asyncio.create_task(
                self._analyze_with_semaphore(article)
            )
            tasks.append(task)
            
        analyzed_high = await asyncio.gather(*tasks)
        
        # ä½å„ªå…ˆåº¦ã¯ç°¡æ˜“åˆ†æ
        analyzed_low = [
            self._simple_analyze(article) 
            for article in low_priority
        ]
        
        return analyzed_high + analyzed_low
        
    async def _analyze_with_semaphore(self, article: Article) -> Article:
        """ã‚»ãƒãƒ•ã‚©åˆ¶å¾¡ä»˜ãåˆ†æ"""
        async with self.semaphore:
            return await self._analyze_article(article)
            
    async def _analyze_article(self, article: Article) -> Article:
        """å˜ä¸€è¨˜äº‹ã®è©³ç´°åˆ†æ"""
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
        cache_key = f"analysis:{hashlib.md5(article.url.encode()).hexdigest()}"
        cached_analysis = await self.cache.get(cache_key)
        
        if cached_analysis:
            return self._apply_analysis(article, cached_analysis)
            
        # Claude APIåˆ†æ
        prompt = self._create_analysis_prompt(article)
        
        try:
            response = await asyncio.to_thread(
                self.client.messages.create,
                model="claude-3-sonnet-20240229",
                max_tokens=1000,
                messages=[{"role": "user", "content": prompt}]
            )
            
            # JSONè§£æ
            content = response.content[0].text
            analysis = json.loads(self._extract_json(content))
            
            # çµæœé©ç”¨
            article.summary = analysis.get('summary', '')
            article.importance_score = analysis.get('importance_score', 5)
            article.keywords = analysis.get('keywords', [])
            article.sentiment = analysis.get('sentiment', 'neutral')
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
            await self.cache.set(cache_key, analysis, ttl=604800)  # 7æ—¥é–“
            
        except Exception as e:
            logger.error(f"Claude analysis error: {e}")
            article = self._simple_analyze(article)
            
        return article
        
    def _create_analysis_prompt(self, article: Article) -> str:
        """åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ"""
        title = article.translated_title or article.title
        content = article.translated_description or article.description
        
        return f"""
ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚

ã‚¿ã‚¤ãƒˆãƒ«: {title}
å†…å®¹: {content}
ã‚«ãƒ†ã‚´ãƒª: {article.category}

ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„:
{{
    "summary": "200-250æ–‡å­—ã®è¦ç´„",
    "importance_score": 1-10ã®é‡è¦åº¦ã‚¹ã‚³ã‚¢,
    "keywords": ["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰3", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰4", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰5"],
    "sentiment": "positive/neutral/negative",
    "key_points": ["ãƒã‚¤ãƒ³ãƒˆ1", "ãƒã‚¤ãƒ³ãƒˆ2", "ãƒã‚¤ãƒ³ãƒˆ3"],
    "urgency": "high/medium/low"
}}

åˆ†æåŸºæº–:
- é‡è¦åº¦: ç¤¾ä¼šå½±éŸ¿åº¦ã€ç·Šæ€¥æ€§ã€é–¢å¿ƒåº¦ã‚’ç·åˆè©•ä¾¡
- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: è¨˜äº‹ã®æ ¸å¿ƒã‚’è¡¨ã™5ã¤ã®é‡è¦èªå¥
- ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆ: è¨˜äº‹ã®è«–èª¿ãƒ»æ„Ÿæƒ…çš„å‚¾å‘
"""
```

### **3. ğŸ“Š Generators (ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå±¤)**

#### **HTMLReportGenerator**
```python
class HTMLReportGenerator:
    """HTMLå½¢å¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    
    def __init__(self, template_dir: str):
        self.env = Environment(
            loader=FileSystemLoader(template_dir),
            autoescape=select_autoescape(['html', 'xml'])
        )
        self.template = self.env.get_template('email_template.html')
        
    def generate(self, articles: List[Article], metadata: dict) -> str:
        """HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        categorized = self._categorize_articles(articles)
        stats = self._generate_statistics(articles)
        alerts = self._extract_urgent_alerts(articles)
        
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰æ•°æº–å‚™
        context = {
            'metadata': metadata,
            'statistics': stats,
            'categories': categorized,
            'urgent_alerts': alerts,
            'generation_time': datetime.now(),
            'total_articles': len(articles),
            'system_info': self._get_system_info()
        }
        
        # HTMLç”Ÿæˆ
        html_content = self.template.render(**context)
        
        # æœ€é©åŒ–
        html_content = self._optimize_html(html_content)
        
        return html_content
        
    def _categorize_articles(self, articles: List[Article]) -> dict:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥è¨˜äº‹æ•´ç†"""
        categories = {
            'ç·Šæ€¥ã‚¢ãƒ©ãƒ¼ãƒˆ': [],
            'å›½å†…ç¤¾ä¼š': [],
            'å›½éš›ç¤¾ä¼š': [],
            'ITãƒ»æŠ€è¡“': [],
            'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£': []
        }
        
        for article in articles:
            # ç·Šæ€¥ã‚¢ãƒ©ãƒ¼ãƒˆåˆ¤å®š
            if article.importance_score >= 9 or article.urgency == 'high':
                categories['ç·Šæ€¥ã‚¢ãƒ©ãƒ¼ãƒˆ'].append(article)
                
            # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
            category_map = {
                'domestic_social': 'å›½å†…ç¤¾ä¼š',
                'international_social': 'å›½éš›ç¤¾ä¼š',
                'tech': 'ITãƒ»æŠ€è¡“',
                'security': 'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£'
            }
            
            category_name = category_map.get(article.category, 'ãã®ä»–')
            if category_name in categories:
                categories[category_name].append(article)
                
        # å„ã‚«ãƒ†ã‚´ãƒªå†…ã§é‡è¦åº¦é †ã‚½ãƒ¼ãƒˆ
        for category in categories:
            categories[category].sort(
                key=lambda x: (x.importance_score, x.published_at),
                reverse=True
            )
            
        return categories
        
    def _generate_statistics(self, articles: List[Article]) -> dict:
        """çµ±è¨ˆæƒ…å ±ç”Ÿæˆ"""
        return {
            'total_count': len(articles),
            'by_category': Counter(article.category for article in articles),
            'by_importance': Counter(article.importance_score for article in articles),
            'by_sentiment': Counter(article.sentiment for article in articles),
            'urgent_count': len([a for a in articles if a.importance_score >= 9]),
            'average_importance': sum(a.importance_score for a in articles) / len(articles),
            'top_keywords': self._extract_top_keywords(articles)
        }
```

---

## ğŸ’¾ **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆ**

### **ERD (Entity Relationship Diagram)**

```mermaid
erDiagram
    ARTICLES {
        integer id PK
        string url UK
        string url_hash
        string title
        string translated_title
        text description
        text content
        text translated_content
        text summary
        string source_name
        string author
        datetime published_at
        datetime collected_at
        string category
        integer importance_score
        json keywords
        string sentiment
        boolean processed
        datetime created_at
    }
    
    DELIVERY_HISTORY {
        integer id PK
        string delivery_type
        string recipient_email
        string subject
        integer article_count
        json categories
        string status
        text error_message
        string html_path
        string pdf_path
        datetime delivered_at
    }
    
    API_USAGE {
        integer id PK
        string api_name
        string endpoint
        integer request_count
        integer response_status
        text error_message
        datetime created_at
    }
    
    CACHE_ENTRIES {
        string cache_key PK
        json value
        datetime expire_at
        datetime created_at
    }
    
    ARTICLES ||--o{ DELIVERY_HISTORY : "includes"
```

### **ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨­è¨ˆ**
```sql
-- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_articles_published_at ON articles(published_at);
CREATE INDEX idx_articles_category ON articles(category);
CREATE INDEX idx_articles_importance ON articles(importance_score DESC);
CREATE INDEX idx_articles_url_hash ON articles(url_hash);
CREATE INDEX idx_articles_processed ON articles(processed);

-- è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_articles_category_importance 
    ON articles(category, importance_score DESC);
CREATE INDEX idx_articles_published_processed 
    ON articles(published_at, processed);

-- é…ä¿¡å±¥æ­´ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_delivery_history_delivered_at 
    ON delivery_history(delivered_at);
CREATE INDEX idx_delivery_history_status 
    ON delivery_history(status);

-- APIä½¿ç”¨å±¥æ­´ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_api_usage_api_name_created 
    ON api_usage(api_name, created_at);

-- ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœŸé™ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX idx_cache_expire_at ON cache_entries(expire_at);
```

---

## ğŸ” **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**

### **å¤šå±¤é˜²å¾¡ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**

```mermaid
graph TB
    subgraph "Application Security"
        A[Input Validation] --> B[Authentication]
        B --> C[Authorization]
        C --> D[Data Encryption]
    end
    
    subgraph "Infrastructure Security"
        E[Environment Variables] --> F[File Permissions]
        F --> G[Network Security]
        G --> H[Access Logging]
    end
    
    subgraph "CI/CD Security"
        I[GitHub Secrets] --> J[Security Scanning]
        J --> K[Dependency Audit]
        K --> L[Code Analysis]
    end
    
    subgraph "Runtime Security"
        M[Error Handling] --> N[Rate Limiting]
        N --> O[Monitoring]
        O --> P[Alerting]
    end
```

### **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å®Ÿè£…è©³ç´°**

#### **èªè¨¼ãƒ»èªå¯**
```python
class SecurityManager:
    """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç®¡ç†"""
    
    @staticmethod
    def validate_api_key(api_key: str, service: str) -> bool:
        """APIã‚­ãƒ¼æ¤œè¨¼"""
        
        # å½¢å¼ãƒã‚§ãƒƒã‚¯
        if not api_key or len(api_key) < 10:
            return False
            
        # ã‚µãƒ¼ãƒ“ã‚¹åˆ¥æ¤œè¨¼
        patterns = {
            'newsapi': r'^[a-f0-9]{32}$',
            'deepl': r'^[a-f0-9-]{36}$',
            'anthropic': r'^sk-ant-[a-zA-Z0-9-]{32,}$'
        }
        
        pattern = patterns.get(service)
        if pattern and not re.match(pattern, api_key):
            return False
            
        return True
        
    @staticmethod
    def sanitize_input(text: str) -> str:
        """å…¥åŠ›ã‚µãƒ‹ã‚¿ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³"""
        
        # HTMLã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
        text = html.escape(text)
        
        # SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–
        text = text.replace("'", "''")
        
        # XSSå¯¾ç­–
        text = re.sub(r'<script[^>]*>.*?</script>', '', text, flags=re.IGNORECASE)
        
        return text
        
    @staticmethod
    def encrypt_sensitive_data(data: str, key: str) -> str:
        """æ©Ÿå¯†ãƒ‡ãƒ¼ã‚¿æš—å·åŒ–"""
        from cryptography.fernet import Fernet
        
        f = Fernet(key.encode())
        encrypted = f.encrypt(data.encode())
        return encrypted.decode()
```

#### **ãƒ¬ãƒ¼ãƒˆåˆ¶é™**
```python
class RateLimiter:
    """API ãƒ¬ãƒ¼ãƒˆåˆ¶é™ç®¡ç†"""
    
    def __init__(self):
        self.limits = {
            'newsapi': {'limit': 1000, 'window': 86400},  # 1000/æ—¥
            'deepl': {'limit': 500000, 'window': 2592000},  # 500kæ–‡å­—/æœˆ
            'claude': {'limit': 1000, 'window': 86400}  # 1000ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/æ—¥
        }
        self.usage = {}
        
    async def check_limit(self, service: str) -> bool:
        """åˆ¶é™ãƒã‚§ãƒƒã‚¯"""
        
        current_time = time.time()
        service_limit = self.limits.get(service, {})
        
        if service not in self.usage:
            self.usage[service] = []
            
        # æœŸé™åˆ‡ã‚Œãƒªã‚¯ã‚¨ã‚¹ãƒˆå‰Šé™¤
        window = service_limit.get('window', 3600)
        cutoff_time = current_time - window
        
        self.usage[service] = [
            timestamp for timestamp in self.usage[service] 
            if timestamp > cutoff_time
        ]
        
        # åˆ¶é™ãƒã‚§ãƒƒã‚¯
        limit = service_limit.get('limit', 100)
        if len(self.usage[service]) >= limit:
            return False
            
        # ä½¿ç”¨é‡è¨˜éŒ²
        self.usage[service].append(current_time)
        return True
```

---

## ğŸ“ˆ **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–**

### **éåŒæœŸå‡¦ç†ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**

```python
class PerformanceOptimizer:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–"""
    
    def __init__(self):
        self.connection_pool = aiohttp.TCPConnector(
            limit=100,  # ç·æ¥ç¶šæ•°
            limit_per_host=30,  # ãƒ›ã‚¹ãƒˆåˆ¥æ¥ç¶šæ•°
            ttl_dns_cache=300,  # DNS ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            use_dns_cache=True
        )
        
    async def parallel_collect(self, collectors: List[BaseCollector]) -> List[Article]:
        """ä¸¦åˆ—ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†"""
        
        # ã‚»ãƒãƒ•ã‚©ã§åŒæ™‚å®Ÿè¡Œæ•°åˆ¶å¾¡
        semaphore = asyncio.Semaphore(6)
        
        async def collect_with_semaphore(collector, category, count):
            async with semaphore:
                return await collector.collect(category, count)
                
        # åé›†ã‚¿ã‚¹ã‚¯ä½œæˆ
        tasks = []
        for collector, config in self._get_collection_tasks():
            task = asyncio.create_task(
                collect_with_semaphore(
                    collector, 
                    config['category'], 
                    config['count']
                )
            )
            tasks.append(task)
            
        # ä¸¦åˆ—å®Ÿè¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # çµæœçµ±åˆ
        all_articles = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"Collection error: {result}")
                continue
            all_articles.extend(result)
            
        return all_articles
```

### **ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥**

```python
class CacheManager:
    """éšå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†"""
    
    def __init__(self):
        # L1: ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ (é«˜é€Ÿ)
        self.memory_cache = {}
        self.memory_ttl = {}
        
        # L2: ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ (æ°¸ç¶š)
        self.disk_cache_dir = Path("cache")
        self.disk_cache_dir.mkdir(exist_ok=True)
        
    async def get(self, key: str) -> Optional[Any]:
        """éšå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—"""
        
        # L1 ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
        if key in self.memory_cache:
            if time.time() < self.memory_ttl.get(key, 0):
                return self.memory_cache[key]
            else:
                del self.memory_cache[key]
                del self.memory_ttl[key]
                
        # L2 ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
        cache_file = self.disk_cache_dir / f"{hashlib.md5(key.encode()).hexdigest()}.json"
        
        if cache_file.exists():
            try:
                with open(cache_file, 'r') as f:
                    cache_data = json.load(f)
                    
                if time.time() < cache_data.get('expire_at', 0):
                    value = cache_data['value']
                    # L1ã«æ˜‡æ ¼
                    self.memory_cache[key] = value
                    self.memory_ttl[key] = cache_data['expire_at']
                    return value
                else:
                    cache_file.unlink()  # æœŸé™åˆ‡ã‚Œãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                    
            except (json.JSONDecodeError, KeyError):
                cache_file.unlink()  # ç ´æãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                
        return None
        
    async def set(self, key: str, value: Any, ttl: int = 3600) -> None:
        """éšå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š"""
        
        expire_at = time.time() + ttl
        
        # L1 ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        self.memory_cache[key] = value
        self.memory_ttl[key] = expire_at
        
        # L2 ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        cache_file = self.disk_cache_dir / f"{hashlib.md5(key.encode()).hexdigest()}.json"
        
        cache_data = {
            'value': value,
            'expire_at': expire_at,
            'created_at': time.time()
        }
        
        with open(cache_file, 'w') as f:
            json.dump(cache_data, f, default=str)
```

---

## ğŸ”„ **GitHub Actions CI/CD ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**

### **ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é€£æºå›³**

```mermaid
graph LR
    A[Code Push] --> B[CI/CD Pipeline]
    A --> C[Security Automation]
    A --> D[PR Automation]
    A --> E[Issue Automation]
    A --> F[Documentation Automation]
    
    B --> G[Test & Build]
    C --> H[Security Scan]
    D --> I[PR Validation]
    E --> J[Issue Triage]
    F --> K[Doc Generation]
    
    G --> L[Quality Gate]
    H --> M[Security Report]
    I --> N[Auto Label/Assign]
    J --> O[Auto Classification]
    K --> P[Auto Commit]
    
    L --> Q[Deploy Decision]
    M --> R[Security Issue]
    N --> S[Review Assignment]
    O --> T[Priority Setting]
    P --> U[Documentation Update]
```

### **å“è³ªã‚²ãƒ¼ãƒˆå®Ÿè£…**

```yaml
# .github/workflows/quality-gate.yml
name: Quality Gate

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]

jobs:
  quality-check:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Security Check
      run: |
        safety check
        bandit -r src/
        
    - name: Code Quality
      run: |
        flake8 src/
        mypy src/
        
    - name: Test Coverage
      run: |
        pytest --cov=src --cov-report=xml
        
    - name: Performance Test
      run: |
        python tests/performance_test.py
        
    - name: Quality Gate Decision
      run: |
        python scripts/quality_gate.py \
          --security-report security-report.json \
          --coverage-report coverage.xml \
          --performance-report performance.json
```

---

## ğŸ“Š **ç›£è¦–ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹**

### **ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­è¨ˆ**

```python
class MetricsCollector:
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
    
    def __init__(self):
        self.metrics = {}
        
    def collect_system_metrics(self) -> dict:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        
        return {
            'timestamp': datetime.now().isoformat(),
            'system': {
                'cpu_usage': psutil.cpu_percent(),
                'memory_usage': psutil.virtual_memory().percent,
                'disk_usage': psutil.disk_usage('/').percent,
                'network_io': psutil.net_io_counters()._asdict()
            },
            'application': {
                'articles_processed': self._count_articles_today(),
                'emails_sent': self._count_emails_today(),
                'api_calls': self._count_api_calls_today(),
                'cache_hit_rate': self._calculate_cache_hit_rate(),
                'error_rate': self._calculate_error_rate()
            },
            'performance': {
                'avg_collection_time': self._avg_collection_time(),
                'avg_processing_time': self._avg_processing_time(),
                'avg_delivery_time': self._avg_delivery_time()
            }
        }
        
    def generate_health_report(self) -> dict:
        """ãƒ˜ãƒ«ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        metrics = self.collect_system_metrics()
        
        health_status = {
            'overall': 'healthy',
            'components': {
                'collectors': self._check_collectors_health(),
                'processors': self._check_processors_health(),
                'generators': self._check_generators_health(),
                'notifiers': self._check_notifiers_health(),
                'database': self._check_database_health()
            },
            'alerts': [],
            'recommendations': []
        }
        
        # ãƒ˜ãƒ«ã‚¹åˆ¤å®š
        if metrics['system']['cpu_usage'] > 80:
            health_status['alerts'].append('High CPU usage detected')
            health_status['overall'] = 'warning'
            
        if metrics['application']['error_rate'] > 0.05:
            health_status['alerts'].append('High error rate detected')
            health_status['overall'] = 'critical'
            
        return health_status
```

---

## ğŸ¯ **æ‹¡å¼µæ€§ã¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æ€§**

### **ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**

```python
class PluginManager:
    """ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.collectors = {}
        self.processors = {}
        self.generators = {}
        self.notifiers = {}
        
    def register_collector(self, name: str, collector_class: Type[BaseCollector]):
        """ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼ç™»éŒ²"""
        self.collectors[name] = collector_class
        
    def register_processor(self, name: str, processor_class: Type[BaseProcessor]):
        """ãƒ—ãƒ­ã‚»ãƒƒã‚µç™»éŒ²"""
        self.processors[name] = processor_class
        
    def create_collector(self, name: str, **kwargs) -> BaseCollector:
        """ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼ç”Ÿæˆ"""
        if name not in self.collectors:
            raise ValueError(f"Unknown collector: {name}")
            
        return self.collectors[name](**kwargs)
        
    def get_available_plugins(self) -> dict:
        """åˆ©ç”¨å¯èƒ½ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ä¸€è¦§"""
        return {
            'collectors': list(self.collectors.keys()),
            'processors': list(self.processors.keys()),
            'generators': list(self.generators.keys()),
            'notifiers': list(self.notifiers.keys())
        }
```

### **è¨­å®šç®¡ç†ã‚·ã‚¹ãƒ†ãƒ **

```python
class ConfigurationManager:
    """è¨­å®šç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config_path: str):
        self.config_path = Path(config_path)
        self.config = {}
        self.watchers = []
        
    def load_config(self) -> dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.config = json.load(f)
                
            # ç’°å¢ƒå¤‰æ•°ã§ã®ä¸Šæ›¸ã
            self._apply_env_overrides()
            
            # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            self._validate_config()
            
            return self.config
            
        except (FileNotFoundError, json.JSONDecodeError) as e:
            logger.error(f"Config load error: {e}")
            raise ConfigurationError(f"Failed to load config: {e}")
            
    def watch_config_changes(self, callback: Callable):
        """è¨­å®šå¤‰æ›´ç›£è¦–"""
        
        class ConfigWatcher(FileSystemEventHandler):
            def on_modified(self, event):
                if event.src_path == str(self.config_path):
                    callback()
                    
        observer = Observer()
        observer.schedule(ConfigWatcher(), str(self.config_path.parent))
        observer.start()
        
        self.watchers.append(observer)
```

ã“ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚Šã€é«˜ã„å¯ç”¨æ€§ã€æ‹¡å¼µæ€§ã€ä¿å®ˆæ€§ã‚’å®Ÿç¾ã—ã€ä¼æ¥­ãƒ¬ãƒ™ãƒ«ã®é‹ç”¨è¦ä»¶ã‚’æº€ãŸã™ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¦ã„ã¾ã™ã€‚

---

**ğŸ¯ ã“ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒªãƒ†ã‚£ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€æ‹¡å¼µæ€§ã®ã™ã¹ã¦ã‚’è€ƒæ…®ã—ãŸè¨­è¨ˆã¨ãªã£ã¦ã„ã¾ã™ã€‚**