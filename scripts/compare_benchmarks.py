#!/usr/bin/env python3
"""
ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®æ¯”è¼ƒã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–æ¤œå‡º
"""
import json
import sys
from pathlib import Path
from typing import Dict, Any, List
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BenchmarkComparator:
    def __init__(self, threshold_percent: float = 20.0):
        """
        Args:
            threshold_percent: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–ã®é–¾å€¤ï¼ˆï¼…ï¼‰
        """
        self.threshold = threshold_percent
        
    def load_benchmark_data(self, filepath: str) -> Dict[str, Any]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(filepath, 'r') as f:
                return json.load(f)
        except (FileNotFoundError, json.JSONDecodeError) as e:
            logger.error(f"Failed to load benchmark data from {filepath}: {e}")
            return {}
    
    def extract_metrics(self, benchmark_data: Dict[str, Any]) -> Dict[str, float]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º"""
        metrics = {}
        
        if 'benchmarks' not in benchmark_data:
            return metrics
            
        for benchmark in benchmark_data['benchmarks']:
            name = benchmark.get('name', 'unknown')
            stats = benchmark.get('stats', {})
            
            # ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
            if 'mean' in stats:
                metrics[f"{name}_mean"] = stats['mean']
            if 'min' in stats:
                metrics[f"{name}_min"] = stats['min']
            if 'max' in stats:
                metrics[f"{name}_max"] = stats['max']
            if 'stddev' in stats:
                metrics[f"{name}_stddev"] = stats['stddev']
                
        return metrics
    
    def compare_metrics(self, baseline: Dict[str, float], current: Dict[str, float]) -> Dict[str, Dict[str, Any]]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ¯”è¼ƒã¨åŠ£åŒ–æ¤œå‡º"""
        results = {}
        
        for metric_name in baseline.keys():
            if metric_name not in current:
                continue
                
            baseline_value = baseline[metric_name]
            current_value = current[metric_name]
            
            if baseline_value == 0:
                continue
                
            # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸å¤‰åŒ–ã‚’è¨ˆç®—
            percent_change = ((current_value - baseline_value) / baseline_value) * 100
            
            # åŠ£åŒ–åˆ¤å®šï¼ˆå®Ÿè¡Œæ™‚é–“ã®å¢—åŠ ã¯åŠ£åŒ–ï¼‰
            is_regression = percent_change > self.threshold
            
            results[metric_name] = {
                'baseline': baseline_value,
                'current': current_value,
                'percent_change': percent_change,
                'is_regression': is_regression,
                'status': 'REGRESSION' if is_regression else 'OK'
            }
            
        return results
    
    def generate_report(self, comparison_results: Dict[str, Dict[str, Any]]) -> str:
        """æ¯”è¼ƒçµæœã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report_lines = []
        report_lines.append("# ğŸ“Š Performance Comparison Report")
        report_lines.append("")
        
        regressions = [k for k, v in comparison_results.items() if v['is_regression']]
        
        if regressions:
            report_lines.append("## âš ï¸ Performance Regressions Detected")
            report_lines.append("")
            report_lines.append("| Metric | Baseline | Current | Change | Status |")
            report_lines.append("|--------|----------|---------|--------|--------|")
            
            for metric in regressions:
                data = comparison_results[metric]
                report_lines.append(
                    f"| {metric} | {data['baseline']:.4f} | {data['current']:.4f} | "
                    f"{data['percent_change']:+.2f}% | {data['status']} |"
                )
        else:
            report_lines.append("## âœ… No Performance Regressions")
            
        report_lines.append("")
        report_lines.append("## ğŸ“ˆ All Metrics")
        report_lines.append("")
        report_lines.append("| Metric | Baseline | Current | Change | Status |")
        report_lines.append("|--------|----------|---------|--------|--------|")
        
        for metric, data in comparison_results.items():
            status_emoji = "âš ï¸" if data['is_regression'] else "âœ…"
            report_lines.append(
                f"| {metric} | {data['baseline']:.4f} | {data['current']:.4f} | "
                f"{data['percent_change']:+.2f}% | {status_emoji} {data['status']} |"
            )
            
        return "\n".join(report_lines)
    
    def run_comparison(self, baseline_file: str, current_file: str) -> bool:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒã®å®Ÿè¡Œ"""
        logger.info(f"Comparing benchmarks: {baseline_file} vs {current_file}")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        baseline_data = self.load_benchmark_data(baseline_file)
        current_data = self.load_benchmark_data(current_file)
        
        if not baseline_data or not current_data:
            logger.error("Failed to load benchmark data")
            return False
            
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º
        baseline_metrics = self.extract_metrics(baseline_data)
        current_metrics = self.extract_metrics(current_data)
        
        if not baseline_metrics or not current_metrics:
            logger.warning("No metrics found in benchmark data")
            return True
            
        # æ¯”è¼ƒå®Ÿè¡Œ
        comparison_results = self.compare_metrics(baseline_metrics, current_metrics)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = self.generate_report(comparison_results)
        
        # ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›
        print(report)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open('performance-comparison-report.md', 'w') as f:
            f.write(report)
            
        # åŠ£åŒ–ãŒã‚ã£ãŸã‹ãƒã‚§ãƒƒã‚¯
        regressions = [k for k, v in comparison_results.items() if v['is_regression']]
        
        if regressions:
            logger.warning(f"Performance regressions detected: {len(regressions)} metrics")
            # GitHub Actionsã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã«è¨­å®š
            with open('performance-regression.txt', 'w') as f:
                f.write(f"REGRESSIONS={len(regressions)}\n")
            return False
        else:
            logger.info("No performance regressions detected")
            return True

def main():
    if len(sys.argv) != 3:
        print("Usage: python compare_benchmarks.py <baseline.json> <current.json>")
        sys.exit(1)
        
    baseline_file = sys.argv[1]
    current_file = sys.argv[2]
    
    comparator = BenchmarkComparator(threshold_percent=20.0)
    success = comparator.run_comparison(baseline_file, current_file)
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()