"""
Performance Benchmark Tests
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ - åŒ…æ‹¬çš„ãªæ€§èƒ½æ¸¬å®šã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
"""

import asyncio
import time
import statistics
import json
import psutil
import gc
from typing import Dict, List, Any, Optional, Callable, Tuple
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from pathlib import Path
import logging
import sys
import os
import random
import string
import concurrent.futures
from contextlib import asynccontextmanager

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from utils.performance_optimizer import OptimizedExecutor, PerformanceMetrics, get_optimizer
from utils.cache_manager_advanced import AdvancedCacheManager, CacheBackend
from utils.async_executor import AsyncExecutor, TaskConfig, TaskPriority
from monitoring.performance_monitor import PerformanceMonitor

logger = logging.getLogger(__name__)

@dataclass
class BenchmarkResult:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ"""
    test_name: str
    start_time: datetime
    end_time: datetime
    duration: float
    iterations: int
    throughput: float  # operations per second
    memory_usage_before: float  # MB
    memory_usage_after: float  # MB
    memory_peak: float  # MB
    cpu_usage_avg: float  # %
    success_rate: float  # 0-1
    errors: List[str] = field(default_factory=list)
    metrics: Dict[str, Any] = field(default_factory=dict)
    
    @property
    def avg_operation_time(self) -> float:
        """å¹³å‡æ“ä½œæ™‚é–“ï¼ˆç§’ï¼‰"""
        return self.duration / max(1, self.iterations)

@dataclass
class BenchmarkSuite:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆ"""
    name: str
    results: List[BenchmarkResult] = field(default_factory=list)
    config: Dict[str, Any] = field(default_factory=dict)
    environment: Dict[str, Any] = field(default_factory=dict)
    
    def add_result(self, result: BenchmarkResult):
        """çµæœè¿½åŠ """
        self.results.append(result)
    
    def get_summary(self) -> Dict[str, Any]:
        """ã‚µãƒãƒªå–å¾—"""
        if not self.results:
            return {}
        
        total_duration = sum(r.duration for r in self.results)
        total_iterations = sum(r.iterations for r in self.results)
        avg_throughput = statistics.mean(r.throughput for r in self.results if r.throughput > 0)
        avg_success_rate = statistics.mean(r.success_rate for r in self.results)
        
        return {
            'total_tests': len(self.results),
            'total_duration': total_duration,
            'total_iterations': total_iterations,
            'avg_throughput': avg_throughput,
            'avg_success_rate': avg_success_rate,
            'memory_peak': max(r.memory_peak for r in self.results),
            'environment': self.environment
        }

class BenchmarkRunner:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.suites = {}
        self.current_suite = None
        
    @asynccontextmanager
    async def benchmark_context(self, test_name: str):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ"""
        # é–‹å§‹å‰ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        start_time = datetime.now()
        initial_memory = psutil.Process().memory_info().rss / 1024 / 1024
        peak_memory = initial_memory
        cpu_samples = []
        
        # CPUç›£è¦–ã‚¿ã‚¹ã‚¯
        monitoring = True
        
        async def monitor_cpu():
            nonlocal peak_memory, cpu_samples, monitoring
            while monitoring:
                try:
                    cpu_percent = psutil.cpu_percent(interval=0.1)
                    cpu_samples.append(cpu_percent)
                    
                    current_memory = psutil.Process().memory_info().rss / 1024 / 1024
                    peak_memory = max(peak_memory, current_memory)
                    
                    await asyncio.sleep(0.1)
                except:
                    break
        
        monitor_task = asyncio.create_task(monitor_cpu())
        
        try:
            yield {
                'start_time': start_time,
                'initial_memory': initial_memory
            }
        finally:
            monitoring = False
            monitor_task.cancel()
            
            end_time = datetime.now()
            final_memory = psutil.Process().memory_info().rss / 1024 / 1024
            avg_cpu = statistics.mean(cpu_samples) if cpu_samples else 0.0
            
            # çµæœãƒ‡ãƒ¼ã‚¿æä¾›
            yield {
                'end_time': end_time,
                'final_memory': final_memory,
                'peak_memory': peak_memory,
                'avg_cpu': avg_cpu,
                'duration': (end_time - start_time).total_seconds()
            }
    
    async def run_benchmark(self, 
                           test_name: str,
                           test_func: Callable,
                           iterations: int = 100,
                           *args, **kwargs) -> BenchmarkResult:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        errors = []
        successful_iterations = 0
        
        async with self.benchmark_context(test_name) as context:
            start_context = await context.__anext__()
            
            for i in range(iterations):
                try:
                    if asyncio.iscoroutinefunction(test_func):
                        await test_func(*args, **kwargs)
                    else:
                        test_func(*args, **kwargs)
                    successful_iterations += 1
                except Exception as e:
                    errors.append(f"Iteration {i}: {str(e)}")
            
            end_context = await context.__anext__()
        
        # çµæœä½œæˆ
        duration = end_context['duration']
        throughput = successful_iterations / duration if duration > 0 else 0
        success_rate = successful_iterations / iterations if iterations > 0 else 0
        
        result = BenchmarkResult(
            test_name=test_name,
            start_time=start_context['start_time'],
            end_time=end_context['end_time'],
            duration=duration,
            iterations=successful_iterations,
            throughput=throughput,
            memory_usage_before=start_context['initial_memory'],
            memory_usage_after=end_context['final_memory'],
            memory_peak=end_context['peak_memory'],
            cpu_usage_avg=end_context['avg_cpu'],
            success_rate=success_rate,
            errors=errors[:10]  # æœ€åˆã®10ã‚¨ãƒ©ãƒ¼ã®ã¿ä¿å­˜
        )
        
        if self.current_suite:
            self.current_suite.add_result(result)
        
        return result
    
    def create_suite(self, name: str, config: Dict[str, Any] = None) -> BenchmarkSuite:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆä½œæˆ"""
        # ç’°å¢ƒæƒ…å ±åé›†
        environment = {
            'python_version': sys.version,
            'platform': sys.platform,
            'cpu_count': psutil.cpu_count(),
            'memory_total_gb': psutil.virtual_memory().total / 1024 / 1024 / 1024,
            'timestamp': datetime.now().isoformat()
        }
        
        suite = BenchmarkSuite(
            name=name,
            config=config or {},
            environment=environment
        )
        
        self.suites[name] = suite
        self.current_suite = suite
        return suite

class PerformanceBenchmarks:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆé›†"""
    
    def __init__(self):
        self.runner = BenchmarkRunner()
        
    async def run_all_benchmarks(self) -> Dict[str, BenchmarkSuite]:
        """å…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        results = {}
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        cache_suite = await self.run_cache_benchmarks()
        results['cache'] = cache_suite
        
        # ä¸¦åˆ—å‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        parallel_suite = await self.run_parallel_benchmarks()
        results['parallel'] = parallel_suite
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        db_suite = await self.run_database_benchmarks()
        results['database'] = db_suite
        
        # APIå‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        api_suite = await self.run_api_benchmarks()
        results['api'] = api_suite
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        memory_suite = await self.run_memory_benchmarks()
        results['memory'] = memory_suite
        
        return results
    
    async def run_cache_benchmarks(self) -> BenchmarkSuite:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        suite = self.runner.create_suite("Cache Performance", {
            'cache_sizes': [100, 500, 1000],
            'backends': ['memory', 'hybrid']
        })
        
        # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ã‚¹ãƒˆ
        memory_cache = AdvancedCacheManager(CacheBackend.MEMORY)
        
        await self.runner.run_benchmark(
            "cache_set_memory",
            self._cache_set_test,
            iterations=1000,
            cache_manager=memory_cache
        )
        
        await self.runner.run_benchmark(
            "cache_get_memory",
            self._cache_get_test,
            iterations=1000,
            cache_manager=memory_cache
        )
        
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ã‚¹ãƒˆ
        hybrid_cache = AdvancedCacheManager(CacheBackend.HYBRID)
        
        await self.runner.run_benchmark(
            "cache_set_hybrid",
            self._cache_set_test,
            iterations=500,
            cache_manager=hybrid_cache
        )
        
        await self.runner.run_benchmark(
            "cache_get_hybrid",
            self._cache_get_test,
            iterations=500,
            cache_manager=hybrid_cache
        )
        
        return suite
    
    async def _cache_set_test(self, cache_manager: AdvancedCacheManager):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®šãƒ†ã‚¹ãƒˆ"""
        key = f"test_key_{random.randint(1, 1000)}"
        value = {
            'data': ''.join(random.choices(string.ascii_letters, k=100)),
            'timestamp': datetime.now().isoformat(),
            'number': random.randint(1, 1000000)
        }
        await cache_manager.set(key, value, ttl=3600)
    
    async def _cache_get_test(self, cache_manager: AdvancedCacheManager):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ãƒ†ã‚¹ãƒˆ"""
        key = f"test_key_{random.randint(1, 1000)}"
        await cache_manager.get(key)
    
    async def run_parallel_benchmarks(self) -> BenchmarkSuite:
        """ä¸¦åˆ—å‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        suite = self.runner.create_suite("Parallel Processing", {
            'worker_counts': [5, 10, 20],
            'task_counts': [100, 500, 1000]
        })
        
        executor = AsyncExecutor(max_workers=10)
        await executor.start()
        
        try:
            # ä¸¦åˆ—ã‚¿ã‚¹ã‚¯å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
            await self.runner.run_benchmark(
                "parallel_light_tasks",
                self._parallel_light_tasks_test,
                iterations=5,
                executor=executor,
                task_count=100
            )
            
            await self.runner.run_benchmark(
                "parallel_heavy_tasks",
                self._parallel_heavy_tasks_test,
                iterations=3,
                executor=executor,
                task_count=50
            )
            
            # ãƒãƒƒãƒå‡¦ç†ãƒ†ã‚¹ãƒˆ
            await self.runner.run_benchmark(
                "batch_processing",
                self._batch_processing_test,
                iterations=5,
                executor=executor
            )
            
        finally:
            await executor.stop()
        
        return suite
    
    async def _parallel_light_tasks_test(self, executor: AsyncExecutor, task_count: int):
        """è»½é‡ä¸¦åˆ—ã‚¿ã‚¹ã‚¯ãƒ†ã‚¹ãƒˆ"""
        async def light_task(x):
            await asyncio.sleep(0.01)  # 10ms
            return x * 2
        
        tasks = []
        for i in range(task_count):
            task_id = await executor.submit_task(f"light_{i}", light_task, (i,))
            tasks.append(task_id)
        
        results = await executor.wait_for_batch(tasks)
        return len([r for r in results if r.is_successful])
    
    async def _parallel_heavy_tasks_test(self, executor: AsyncExecutor, task_count: int):
        """é‡ã„ä¸¦åˆ—ã‚¿ã‚¹ã‚¯ãƒ†ã‚¹ãƒˆ"""
        def heavy_task(x):
            # CPUé›†ç´„çš„ã‚¿ã‚¹ã‚¯
            result = 0
            for i in range(100000):
                result += i * x
            return result
        
        config = TaskConfig(use_thread_pool=True)
        tasks = []
        for i in range(task_count):
            task_id = await executor.submit_task(f"heavy_{i}", heavy_task, (i,), config=config)
            tasks.append(task_id)
        
        results = await executor.wait_for_batch(tasks)
        return len([r for r in results if r.is_successful])
    
    async def _batch_processing_test(self, executor: AsyncExecutor):
        """ãƒãƒƒãƒå‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        items = list(range(1000))
        
        async def process_batch(batch):
            await asyncio.sleep(0.1)  # æ¨¡æ“¬å‡¦ç†æ™‚é–“
            return [x * 2 for x in batch]
        
        results = await executor.process_parallel_batches(
            items, process_batch, batch_size=50, max_concurrent=5
        )
        
        return len(results)
    
    async def run_database_benchmarks(self) -> BenchmarkSuite:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        suite = self.runner.create_suite("Database Performance", {
            'operations': ['insert', 'select', 'update', 'delete'],
            'batch_sizes': [10, 50, 100]
        })
        
        # SQLiteãƒ†ã‚¹ãƒˆï¼ˆã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªï¼‰
        import sqlite3
        
        conn = sqlite3.connect(':memory:')
        conn.execute('''
            CREATE TABLE test_table (
                id INTEGER PRIMARY KEY,
                data TEXT,
                value INTEGER,
                timestamp TEXT
            )
        ''')
        
        await self.runner.run_benchmark(
            "db_insert_single",
            self._db_insert_test,
            iterations=1000,
            connection=conn,
            batch_size=1
        )
        
        await self.runner.run_benchmark(
            "db_insert_batch",
            self._db_insert_test,
            iterations=100,
            connection=conn,
            batch_size=10
        )
        
        await self.runner.run_benchmark(
            "db_select",
            self._db_select_test,
            iterations=1000,
            connection=conn
        )
        
        conn.close()
        return suite
    
    def _db_insert_test(self, connection, batch_size: int = 1):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŒ¿å…¥ãƒ†ã‚¹ãƒˆ"""
        data_batch = []
        for i in range(batch_size):
            data_batch.append((
                f"test_data_{random.randint(1, 10000)}",
                random.randint(1, 1000),
                datetime.now().isoformat()
            ))
        
        if batch_size == 1:
            connection.execute(
                'INSERT INTO test_table (data, value, timestamp) VALUES (?, ?, ?)',
                data_batch[0]
            )
        else:
            connection.executemany(
                'INSERT INTO test_table (data, value, timestamp) VALUES (?, ?, ?)',
                data_batch
            )
        connection.commit()
    
    def _db_select_test(self, connection):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é¸æŠãƒ†ã‚¹ãƒˆ"""
        cursor = connection.execute(
            'SELECT * FROM test_table WHERE value > ? LIMIT 10',
            (random.randint(1, 500),)
        )
        return cursor.fetchall()
    
    async def run_api_benchmarks(self) -> BenchmarkSuite:
        """APIå‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        suite = self.runner.create_suite("API Processing", {
            'simulated_apis': ['news', 'translation', 'analysis'],
            'concurrent_requests': [1, 5, 10]
        })
        
        await self.runner.run_benchmark(
            "api_news_simulation",
            self._api_simulation_test,
            iterations=100,
            api_type="news",
            response_time=0.2
        )
        
        await self.runner.run_benchmark(
            "api_translation_simulation",
            self._api_simulation_test,
            iterations=50,
            api_type="translation",
            response_time=0.5
        )
        
        await self.runner.run_benchmark(
            "api_analysis_simulation",
            self._api_simulation_test,
            iterations=30,
            api_type="analysis",
            response_time=1.0
        )
        
        return suite
    
    async def _api_simulation_test(self, api_type: str, response_time: float):
        """APIå‘¼ã³å‡ºã—ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        # æ¨¡æ“¬ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“
        await asyncio.sleep(response_time + random.uniform(-0.1, 0.1))
        
        # æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        if api_type == "news":
            return {
                'articles': [
                    {
                        'title': f'Article {i}',
                        'content': ''.join(random.choices(string.ascii_letters, k=500))
                    }
                    for i in range(10)
                ]
            }
        elif api_type == "translation":
            return {
                'translated_text': ''.join(random.choices(string.ascii_letters, k=200))
            }
        elif api_type == "analysis":
            return {
                'summary': ''.join(random.choices(string.ascii_letters, k=150)),
                'keywords': [f'keyword_{i}' for i in range(5)],
                'importance': random.randint(1, 10)
            }
    
    async def run_memory_benchmarks(self) -> BenchmarkSuite:
        """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        suite = self.runner.create_suite("Memory Efficiency", {
            'data_sizes': ['small', 'medium', 'large'],
            'operations': ['create', 'process', 'cleanup']
        })
        
        await self.runner.run_benchmark(
            "memory_large_data_processing",
            self._memory_large_data_test,
            iterations=10
        )
        
        await self.runner.run_benchmark(
            "memory_generator_vs_list",
            self._memory_generator_test,
            iterations=5
        )
        
        await self.runner.run_benchmark(
            "memory_gc_efficiency",
            self._memory_gc_test,
            iterations=20
        )
        
        return suite
    
    def _memory_large_data_test(self):
        """å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        # å¤§ããªãƒ‡ãƒ¼ã‚¿æ§‹é€ ä½œæˆ
        data = [
            {
                'id': i,
                'content': ''.join(random.choices(string.ascii_letters, k=1000)),
                'numbers': [random.randint(1, 1000) for _ in range(100)]
            }
            for i in range(1000)
        ]
        
        # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
        processed = []
        for item in data:
            processed.append({
                'id': item['id'],
                'content_length': len(item['content']),
                'number_sum': sum(item['numbers'])
            })
        
        return len(processed)
    
    def _memory_generator_test(self):
        """ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ vs ãƒªã‚¹ãƒˆ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ†ã‚¹ãƒˆ"""
        def data_generator():
            for i in range(10000):
                yield ''.join(random.choices(string.ascii_letters, k=100))
        
        # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ä½¿ç”¨
        gen_result = sum(len(item) for item in data_generator())
        
        # ãƒªã‚¹ãƒˆä½¿ç”¨
        data_list = [
            ''.join(random.choices(string.ascii_letters, k=100))
            for i in range(10000)
        ]
        list_result = sum(len(item) for item in data_list)
        
        return gen_result == list_result
    
    def _memory_gc_test(self):
        """ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åŠ¹ç‡ãƒ†ã‚¹ãƒˆ"""
        # ä¸€æ™‚çš„ãªå¤§ããªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
        temp_data = []
        for i in range(1000):
            temp_data.append({
                'data': ''.join(random.choices(string.ascii_letters, k=500))
            })
        
        # æ˜ç¤ºçš„ãªå‰Šé™¤
        del temp_data
        
        # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
        collected = gc.collect()
        
        return collected

class BenchmarkReporter:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    
    @staticmethod
    def generate_report(suites: Dict[str, BenchmarkSuite], output_path: str = None) -> Dict[str, Any]:
        """ç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = {
            'generated_at': datetime.now().isoformat(),
            'summary': {},
            'suites': {},
            'recommendations': []
        }
        
        total_tests = 0
        total_duration = 0.0
        all_success_rates = []
        
        for suite_name, suite in suites.items():
            suite_summary = suite.get_summary()
            report['suites'][suite_name] = {
                'summary': suite_summary,
                'results': [asdict(result) for result in suite.results]
            }
            
            total_tests += suite_summary.get('total_tests', 0)
            total_duration += suite_summary.get('total_duration', 0)
            if suite_summary.get('avg_success_rate'):
                all_success_rates.append(suite_summary['avg_success_rate'])
        
        # ç·åˆã‚µãƒãƒª
        report['summary'] = {
            'total_test_suites': len(suites),
            'total_tests': total_tests,
            'total_duration': total_duration,
            'overall_success_rate': statistics.mean(all_success_rates) if all_success_rates else 0,
            'fastest_test': BenchmarkReporter._find_fastest_test(suites),
            'slowest_test': BenchmarkReporter._find_slowest_test(suites)
        }
        
        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        report['recommendations'] = BenchmarkReporter._generate_recommendations(suites)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
        
        return report
    
    @staticmethod
    def _find_fastest_test(suites: Dict[str, BenchmarkSuite]) -> Dict[str, Any]:
        """æœ€é«˜é€Ÿãƒ†ã‚¹ãƒˆæ¤œç´¢"""
        fastest = None
        best_throughput = 0
        
        for suite_name, suite in suites.items():
            for result in suite.results:
                if result.throughput > best_throughput:
                    best_throughput = result.throughput
                    fastest = {
                        'suite': suite_name,
                        'test': result.test_name,
                        'throughput': result.throughput,
                        'avg_time': result.avg_operation_time
                    }
        
        return fastest
    
    @staticmethod
    def _find_slowest_test(suites: Dict[str, BenchmarkSuite]) -> Dict[str, Any]:
        """æœ€ä½é€Ÿãƒ†ã‚¹ãƒˆæ¤œç´¢"""
        slowest = None
        worst_throughput = float('inf')
        
        for suite_name, suite in suites.items():
            for result in suite.results:
                if 0 < result.throughput < worst_throughput:
                    worst_throughput = result.throughput
                    slowest = {
                        'suite': suite_name,
                        'test': result.test_name,
                        'throughput': result.throughput,
                        'avg_time': result.avg_operation_time
                    }
        
        return slowest
    
    @staticmethod
    def _generate_recommendations(suites: Dict[str, BenchmarkSuite]) -> List[str]:
        """æœ€é©åŒ–æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        for suite_name, suite in suites.items():
            if suite_name == 'cache':
                cache_recommendations = BenchmarkReporter._analyze_cache_performance(suite)
                recommendations.extend(cache_recommendations)
            elif suite_name == 'parallel':
                parallel_recommendations = BenchmarkReporter._analyze_parallel_performance(suite)
                recommendations.extend(parallel_recommendations)
            elif suite_name == 'memory':
                memory_recommendations = BenchmarkReporter._analyze_memory_performance(suite)
                recommendations.extend(memory_recommendations)
        
        return recommendations
    
    @staticmethod
    def _analyze_cache_performance(suite: BenchmarkSuite) -> List[str]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ"""
        recommendations = []
        
        memory_results = [r for r in suite.results if 'memory' in r.test_name]
        hybrid_results = [r for r in suite.results if 'hybrid' in r.test_name]
        
        if memory_results and hybrid_results:
            memory_avg = statistics.mean(r.throughput for r in memory_results)
            hybrid_avg = statistics.mean(r.throughput for r in hybrid_results)
            
            if memory_avg > hybrid_avg * 1.5:
                recommendations.append("ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ˆã‚Šå¤§å¹…ã«é«˜é€Ÿã§ã™ã€‚Redisè¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            elif hybrid_avg > memory_avg * 1.2:
                recommendations.append("ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒåŠ¹æœçš„ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚æœ¬ç•ªç’°å¢ƒã§ã®æ¡ç”¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        
        return recommendations
    
    @staticmethod
    def _analyze_parallel_performance(suite: BenchmarkSuite) -> List[str]:
        """ä¸¦åˆ—å‡¦ç†ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ"""
        recommendations = []
        
        light_results = [r for r in suite.results if 'light' in r.test_name]
        heavy_results = [r for r in suite.results if 'heavy' in r.test_name]
        
        if heavy_results:
            heavy_avg_memory = statistics.mean(r.memory_peak for r in heavy_results)
            if heavy_avg_memory > 1000:  # 1GBä»¥ä¸Š
                recommendations.append("é‡ã„ä¸¦åˆ—ã‚¿ã‚¹ã‚¯ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤šã„ã§ã™ã€‚ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ã®ã‚µã‚¤ã‚ºã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
        
        if light_results:
            light_avg_cpu = statistics.mean(r.cpu_usage_avg for r in light_results)
            if light_avg_cpu < 30:
                recommendations.append("è»½é‡ã‚¿ã‚¹ã‚¯ã§CPUä½¿ç”¨ç‡ãŒä½ã„ã§ã™ã€‚ä¸¦åˆ—åº¦ã‚’ä¸Šã’ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚")
        
        return recommendations
    
    @staticmethod
    def _analyze_memory_performance(suite: BenchmarkSuite) -> List[str]:
        """ãƒ¡ãƒ¢ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ"""
        recommendations = []
        
        for result in suite.results:
            memory_increase = result.memory_usage_after - result.memory_usage_before
            if memory_increase > 100:  # 100MBä»¥ä¸Šå¢—åŠ 
                recommendations.append(f"{result.test_name}ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒ{memory_increase:.1f}MBå¢—åŠ ã—ã¾ã—ãŸã€‚ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            
            if result.memory_peak > 1000:  # 1GBä»¥ä¸Š
                recommendations.append(f"{result.test_name}ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ãƒ”ãƒ¼ã‚¯ãŒ{result.memory_peak:.1f}MBã§ã—ãŸã€‚å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
        
        return recommendations

async def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
    print("ğŸš€ Performance Benchmark Tests Starting...")
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    benchmarks = PerformanceBenchmarks()
    
    try:
        results = await benchmarks.run_all_benchmarks()
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report_path = f"benchmark_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report = BenchmarkReporter.generate_report(results, report_path)
        
        # çµæœè¡¨ç¤º
        print("\nğŸ“Š Benchmark Results Summary:")
        print(f"Total test suites: {report['summary']['total_test_suites']}")
        print(f"Total tests: {report['summary']['total_tests']}")
        print(f"Total duration: {report['summary']['total_duration']:.2f}s")
        print(f"Overall success rate: {report['summary']['overall_success_rate']:.2%}")
        
        if report['summary']['fastest_test']:
            fastest = report['summary']['fastest_test']
            print(f"\nFastest test: {fastest['test']} ({fastest['throughput']:.2f} ops/sec)")
        
        if report['summary']['slowest_test']:
            slowest = report['summary']['slowest_test']
            print(f"Slowest test: {slowest['test']} ({slowest['throughput']:.2f} ops/sec)")
        
        print(f"\nğŸ“‹ Recommendations ({len(report['recommendations'])}):")
        for i, rec in enumerate(report['recommendations'], 1):
            print(f"{i}. {rec}")
        
        print(f"\nğŸ“„ Detailed report saved to: {report_path}")
        
    except Exception as e:
        logger.error(f"Benchmark execution failed: {str(e)}")
        raise

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())